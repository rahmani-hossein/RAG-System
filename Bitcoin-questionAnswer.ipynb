{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question-Answering-form-Article\n",
    "This is a quick demo for Question Answering from Articles that I chose famous bitcoin paper by Satoshi Nakamoto. Here, I implemented a RAG system and a using search engine Tavily as an agent to search net if your question isn't in the paper :)))\n",
    "\n",
    "I chose **ChatGPT4** model as my model  and developed it with **LangChain** and **Choroma** which are really strong combination and did great on these questions.\n",
    "\n",
    "For an introduction to RAG, you can check [this other cookbook](https://huggingface.co/learn/cookbook/en/rag_zephyr_langchain)! Also i would give comments about my code that I think it's helpful.\n",
    "\n",
    "RAG systems are complex: here a RAG diagram, where we noted in blue all possibilities for system enhancement:\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface/cookbook-images/resolve/main/RAG_workflow.png\" height=\"700\">\n",
    "\n",
    "Implementing any of these improvements can bring a huge performance boost; but changing anything is useless if you cannot monitor the impact of your changes on the system's performance! \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Future Contributions\n",
    "1. I will add some evaluation of answering quality by building a synthetic evaluation dataset and using LLM-as-a-judge to compute the accuracy of your system.\n",
    "\n",
    "2. Add other resources and make it challenging that vector search fails and use query expansion technique and other on large scale data.\n",
    "\n",
    "\n",
    "**Remark:** I want to treat this as an exercise and implement every interesting method or trick that I find helpful here to have them all in one place. Maybe some people can benefit from them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\hossein rahmani\\pycharmprojects\\chatgpt_prompting\\.venv\\lib\\site-packages (1.14.1)\n",
      "Requirement already satisfied: pypdf in c:\\users\\hossein rahmani\\pycharmprojects\\chatgpt_prompting\\.venv\\lib\\site-packages (4.1.0)\n",
      "Requirement already satisfied: Langchain in c:\\users\\hossein rahmani\\pycharmprojects\\chatgpt_prompting\\.venv\\lib\\site-packages (0.1.12)\n",
      "Collecting chromadb\n",
      "  Downloading chromadb-0.4.24-py3-none-any.whl (525 kB)\n",
      "                                              0.0/525.5 kB ? eta -:--:--\n",
      "     -----------------------                327.7/525.5 kB 6.9 MB/s eta 0:00:01\n",
      "     -------------------------------------  522.2/525.5 kB 8.3 MB/s eta 0:00:01\n",
      "     -------------------------------------- 525.5/525.5 kB 4.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\hossein rahmani\\pycharmprojects\\chatgpt_prompting\\.venv\\lib\\site-packages (from openai) (4.3.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\hossein rahmani\\pycharmprojects\\chatgpt_prompting\\.venv\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\hossein rahmani\\pycharmprojects\\chatgpt_prompting\\.venv\\lib\\site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\hossein rahmani\\pycharmprojects\\chatgpt_prompting\\.venv\\lib\\site-packages (from openai) (2.6.4)\n",
      "Requirement already satisfied: sniffio in c:\\users\\hossein rahmani\\pycharmprojects\\chatgpt_prompting\\.venv\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\hossein rahmani\\pycharmprojects\\chatgpt_prompting\\.venv\\lib\\site-packages (from openai) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\hossein rahmani\\pycharmprojects\\chatgpt_prompting\\.venv\\lib\\site-packages (from openai) (4.10.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\hossein rahmani\\pycharmprojects\\chatgpt_prompting\\.venv\\lib\\site-packages (from Langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\hossein rahmani\\pycharmprojects\\chatgpt_prompting\\.venv\\lib\\site-packages (from Langchain) (2.0.28)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\hossein rahmani\\pycharmprojects\\chatgpt_prompting\\.venv\\lib\\site-packages (from Langchain) (3.9.3)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in c:\\users\\hossein rahmani\\pycharmprojects\\chatgpt_prompting\\.venv\\lib\\site-packages (from Langchain) (4.0.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\hossein rahmani\\pycharmprojects\\chatgpt_prompting\\.venv\\lib\\site-packages (from Langchain) (0.6.4)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\hossein rahmani\\pycharmprojects\\chatgpt_prompting\\.venv\\lib\\site-packages (from Langchain) (1.33)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.28 in c:\\users\\hossein rahmani\\pycharmprojects\\chatgpt_prompting\\.venv\\lib\\site-packages (from Langchain) (0.0.28)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.31 in c:\\users\\hossein rahmani\\pycharmprojects\\chatgpt_prompting\\.venv\\lib\\site-packages (from Langchain) (0.1.32)\n",
      "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in c:\\users\\hossein rahmani\\pycharmprojects\\chatgpt_prompting\\.venv\\lib\\site-packages (from Langchain) (0.0.1)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in c:\\users\\hossein rahmani\\pycharmprojects\\chatgpt_prompting\\.venv\\lib\\site-packages (from Langchain) (0.1.27)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\hossein rahmani\\pycharmprojects\\chatgpt_prompting\\.venv\\lib\\site-packages (from Langchain) (1.26.2)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\hossein rahmani\\pycharmprojects\\chatgpt_prompting\\.venv\\lib\\site-packages (from Langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\hossein rahmani\\pycharmprojects\\chatgpt_prompting\\.venv\\lib\\site-packages (from Langchain) (8.2.3)\n",
      "Collecting build>=1.0.3 (from chromadb)\n",
      "  Downloading build-1.2.1-py3-none-any.whl (21 kB)\n",
      "Collecting chroma-hnswlib==0.7.3 (from chromadb)\n",
      "  Downloading chroma_hnswlib-0.7.3-cp310-cp310-win_amd64.whl (150 kB)\n",
      "                                              0.0/150.6 kB ? eta -:--:--\n",
      "     -------------------------------------- 150.6/150.6 kB 4.4 MB/s eta 0:00:00\n",
      "Collecting fastapi>=0.95.2 (from chromadb)\n",
      "  Downloading fastapi-0.110.0-py3-none-any.whl (92 kB)\n",
      "                                              0.0/92.1 kB ? eta -:--:--\n",
      "     ---------------------------------------- 92.1/92.1 kB 5.1 MB/s eta 0:00:00\n",
      "Collecting uvicorn[standard]>=0.18.3 (from chromadb)\n",
      "  Downloading uvicorn-0.29.0-py3-none-any.whl (60 kB)\n",
      "                                              0.0/60.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 60.8/60.8 kB 3.2 MB/s eta 0:00:00\n",
      "Collecting posthog>=2.4.0 (from chromadb)\n",
      "  Downloading posthog-3.5.0-py2.py3-none-any.whl (41 kB)\n",
      "                                              0.0/41.3 kB ? eta -:--:--\n",
      "     ---------------------------------------- 41.3/41.3 kB ? eta 0:00:00\n",
      "Collecting pulsar-client>=3.1.0 (from chromadb)\n",
      "  Downloading pulsar_client-3.4.0-cp310-cp310-win_amd64.whl (3.4 MB)\n",
      "                                              0.0/3.4 MB ? eta -:--:--\n",
      "     ---------                                0.8/3.4 MB 17.8 MB/s eta 0:00:01\n",
      "     ------------                             1.0/3.4 MB 16.7 MB/s eta 0:00:01\n",
      "     ------------                             1.0/3.4 MB 16.7 MB/s eta 0:00:01\n",
      "     ---------------                          1.3/3.4 MB 6.5 MB/s eta 0:00:01\n",
      "     ------------------------                 2.1/3.4 MB 8.9 MB/s eta 0:00:01\n",
      "     ------------------------                 2.1/3.4 MB 8.9 MB/s eta 0:00:01\n",
      "     ------------------------                 2.1/3.4 MB 8.9 MB/s eta 0:00:01\n",
      "     --------------------------               2.3/3.4 MB 5.8 MB/s eta 0:00:01\n",
      "     --------------------------------         2.8/3.4 MB 6.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------  3.4/3.4 MB 7.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 3.4/3.4 MB 6.1 MB/s eta 0:00:00\n",
      "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
      "  Downloading onnxruntime-1.17.1-cp310-cp310-win_amd64.whl (5.6 MB)\n",
      "                                              0.0/5.6 MB ? eta -:--:--\n",
      "     -------                                  1.1/5.6 MB 34.0 MB/s eta 0:00:01\n",
      "     --------------                           2.0/5.6 MB 25.6 MB/s eta 0:00:01\n",
      "     -----------------                        2.5/5.6 MB 19.7 MB/s eta 0:00:01\n",
      "     ---------------------------              3.8/5.6 MB 22.1 MB/s eta 0:00:01\n",
      "     -----------------------------            4.2/5.6 MB 20.6 MB/s eta 0:00:01\n",
      "     -----------------------------            4.2/5.6 MB 20.6 MB/s eta 0:00:01\n",
      "     -----------------------------            4.2/5.6 MB 20.6 MB/s eta 0:00:01\n",
      "     --------------------------------         4.6/5.6 MB 12.8 MB/s eta 0:00:01\n",
      "     -------------------------------------    5.3/5.6 MB 12.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------  5.6/5.6 MB 12.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  5.6/5.6 MB 12.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 5.6/5.6 MB 10.2 MB/s eta 0:00:00\n",
      "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_api-1.24.0-py3-none-any.whl (60 kB)\n",
      "                                              0.0/60.1 kB ? eta -:--:--\n",
      "     ---------------------------------------- 60.1/60.1 kB 1.6 MB/s eta 0:00:00\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.24.0-py3-none-any.whl (18 kB)\n",
      "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
      "  Downloading opentelemetry_instrumentation_fastapi-0.45b0-py3-none-any.whl (11 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_sdk-1.24.0-py3-none-any.whl (106 kB)\n",
      "                                              0.0/106.1 kB ? eta -:--:--\n",
      "     -------------------------------------- 106.1/106.1 kB 3.1 MB/s eta 0:00:00\n",
      "Collecting tokenizers>=0.13.2 (from chromadb)\n",
      "  Downloading tokenizers-0.15.2-cp310-none-win_amd64.whl (2.2 MB)\n",
      "                                              0.0/2.2 MB ? eta -:--:--\n",
      "     ----------------------                   1.2/2.2 MB 25.7 MB/s eta 0:00:01\n",
      "     ------------------------------------     2.0/2.2 MB 21.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.2/2.2 MB 20.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.2/2.2 MB 14.0 MB/s eta 0:00:00\n",
      "Collecting pypika>=0.48.9 (from chromadb)\n",
      "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
      "                                              0.0/67.3 kB ? eta -:--:--\n",
      "     ------------------------------------     61.4/67.3 kB ? eta -:--:--\n",
      "     ---------------------------------------- 67.3/67.3 kB 1.2 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting overrides>=7.3.1 (from chromadb)\n",
      "  Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
      "Collecting importlib-resources (from chromadb)\n",
      "  Downloading importlib_resources-6.4.0-py3-none-any.whl (38 kB)\n",
      "Collecting grpcio>=1.58.0 (from chromadb)\n",
      "  Downloading grpcio-1.62.1-cp310-cp310-win_amd64.whl (3.8 MB)\n",
      "                                              0.0/3.8 MB ? eta -:--:--\n",
      "     --------                                 0.8/3.8 MB 16.8 MB/s eta 0:00:01\n",
      "     ---------------------                    2.0/3.8 MB 21.8 MB/s eta 0:00:01\n",
      "     -------------------------------          3.0/3.8 MB 21.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  3.8/3.8 MB 20.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  3.8/3.8 MB 20.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 3.8/3.8 MB 13.5 MB/s eta 0:00:00\n",
      "Collecting bcrypt>=4.0.1 (from chromadb)\n",
      "  Downloading bcrypt-4.1.2-cp39-abi3-win_amd64.whl (158 kB)\n",
      "                                              0.0/158.3 kB ? eta -:--:--\n",
      "     -------------------------------------- 158.3/158.3 kB 4.8 MB/s eta 0:00:00\n",
      "Collecting typer>=0.9.0 (from chromadb)\n",
      "  Downloading typer-0.12.0-py3-none-any.whl (5.6 kB)\n",
      "Collecting kubernetes>=28.1.0 (from chromadb)\n",
      "  Downloading kubernetes-29.0.0-py2.py3-none-any.whl (1.6 MB)\n",
      "                                              0.0/1.6 MB ? eta -:--:--\n",
      "     -----------------------                  0.9/1.6 MB 19.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------  1.6/1.6 MB 20.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------  1.6/1.6 MB 14.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.6/1.6 MB 9.2 MB/s eta 0:00:00\n",
      "Collecting mmh3>=4.0.1 (from chromadb)\n",
      "  Downloading mmh3-4.1.0-cp310-cp310-win_amd64.whl (31 kB)\n",
      "Requirement already satisfied: orjson>=3.9.12 in c:\\users\\hossein rahmani\\pycharmprojects\\chatgpt_prompting\\.venv\\lib\\site-packages (from chromadb) (3.9.15)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\hossein rahmani\\pycharmprojects\\chatgpt_prompting\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->Langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\hossein rahmani\\pycharmprojects\\chatgpt_prompting\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->Langchain) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\hossein rahmani\\pycharmprojects\\chatgpt_prompting\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->Langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\hossein rahmani\\pycharmprojects\\chatgpt_prompting\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->Langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\hossein rahmani\\pycharmprojects\\chatgpt_prompting\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->Langchain) (1.9.4)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\hossein rahmani\\pycharmprojects\\chatgpt_prompting\\.venv\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\hossein rahmani\\pycharmprojects\\chatgpt_prompting\\.venv\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
      "Requirement already satisfied: packaging>=19.1 in c:\\users\\hossein rahmani\\pycharmprojects\\chatgpt_prompting\\.venv\\lib\\site-packages (from build>=1.0.3->chromadb) (23.2)\n",
      "Collecting pyproject_hooks (from build>=1.0.3->chromadb)\n",
      "  Downloading pyproject_hooks-1.0.0-py3-none-any.whl (9.3 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\hossein rahmani\\pycharmprojects\\chatgpt_prompting\\.venv\\lib\\site-packages (from build>=1.0.3->chromadb) (0.4.6)\n",
      "Collecting importlib-metadata>=4.6 (from build>=1.0.3->chromadb)\n",
      "  Downloading importlib_metadata-7.1.0-py3-none-any.whl (24 kB)\n",
      "Collecting tomli>=1.1.0 (from build>=1.0.3->chromadb)\n",
      "  Using cached tomli-2.0.1-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\hossein rahmani\\pycharmprojects\\chatgpt_prompting\\.venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->Langchain) (3.21.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\hossein rahmani\\pycharmprojects\\chatgpt_prompting\\.venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->Langchain) (0.9.0)\n",
      "Collecting starlette<0.37.0,>=0.36.3 (from fastapi>=0.95.2->chromadb)\n",
      "  Downloading starlette-0.36.3-py3-none-any.whl (71 kB)\n",
      "                                              0.0/71.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 71.5/71.5 kB 3.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: certifi in c:\\users\\hossein rahmani\\pycharmprojects\\chatgpt_prompting\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\hossein rahmani\\pycharmprojects\\chatgpt_prompting\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.4)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\hossein rahmani\\pycharmprojects\\chatgpt_prompting\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\hossein rahmani\\pycharmprojects\\chatgpt_prompting\\.venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->Langchain) (2.4)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\hossein rahmani\\pycharmprojects\\chatgpt_prompting\\.venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\hossein rahmani\\pycharmprojects\\chatgpt_prompting\\.venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
      "Collecting google-auth>=1.0.1 (from kubernetes>=28.1.0->chromadb)\n",
      "  Downloading google_auth-2.29.0-py2.py3-none-any.whl (189 kB)\n",
      "                                              0.0/189.2 kB ? eta -:--:--\n",
      "     ------------------------------------- 189.2/189.2 kB 11.2 MB/s eta 0:00:00\n",
      "Collecting websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 (from kubernetes>=28.1.0->chromadb)\n",
      "  Using cached websocket_client-1.7.0-py3-none-any.whl (58 kB)\n",
      "Collecting requests-oauthlib (from kubernetes>=28.1.0->chromadb)\n",
      "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Collecting oauthlib>=3.2.2 (from kubernetes>=28.1.0->chromadb)\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "                                              0.0/151.7 kB ? eta -:--:--\n",
      "     -------------------------------------- 151.7/151.7 kB 4.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: urllib3>=1.24.2 in c:\\users\\hossein rahmani\\pycharmprojects\\chatgpt_prompting\\.venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.2.1)\n",
      "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "                                              0.0/46.0 kB ? eta -:--:--\n",
      "     -----------------------------------      41.0/46.0 kB ? eta -:--:--\n",
      "     -------------------------------------- 46.0/46.0 kB 567.9 kB/s eta 0:00:00\n",
      "Collecting flatbuffers (from onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Collecting protobuf (from onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading protobuf-5.26.1-cp310-abi3-win_amd64.whl (420 kB)\n",
      "                                              0.0/420.9 kB ? eta -:--:--\n",
      "     ------------------------------------- 420.9/420.9 kB 27.4 MB/s eta 0:00:00\n",
      "Collecting sympy (from onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "                                              0.0/5.7 MB ? eta -:--:--\n",
      "     ----------------                         2.3/5.7 MB 49.7 MB/s eta 0:00:01\n",
      "     ------------------------                 3.5/5.7 MB 36.9 MB/s eta 0:00:01\n",
      "     -------------------------------------    5.3/5.7 MB 38.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  5.7/5.7 MB 36.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 5.7/5.7 MB 26.2 MB/s eta 0:00:00\n",
      "Collecting deprecated>=1.2.6 (from opentelemetry-api>=1.2.0->chromadb)\n",
      "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting importlib-metadata>=4.6 (from build>=1.0.3->chromadb)\n",
      "  Downloading importlib_metadata-7.0.0-py3-none-any.whl (23 kB)\n",
      "Collecting googleapis-common-protos~=1.52 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading googleapis_common_protos-1.63.0-py2.py3-none-any.whl (229 kB)\n",
      "                                              0.0/229.1 kB ? eta -:--:--\n",
      "     -------------------------------------- 229.1/229.1 kB 7.1 MB/s eta 0:00:00\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.24.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.24.0-py3-none-any.whl (17 kB)\n",
      "Collecting opentelemetry-proto==1.24.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_proto-1.24.0-py3-none-any.whl (50 kB)\n",
      "                                              0.0/50.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 50.8/50.8 kB 2.5 MB/s eta 0:00:00\n",
      "Collecting protobuf (from onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading protobuf-4.25.3-cp310-abi3-win_amd64.whl (413 kB)\n",
      "                                              0.0/413.4 kB ? eta -:--:--\n",
      "     ------------------------------------  409.6/413.4 kB 25.0 MB/s eta 0:00:01\n",
      "     -------------------------------------- 413.4/413.4 kB 8.6 MB/s eta 0:00:00\n",
      "Collecting opentelemetry-instrumentation-asgi==0.45b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_instrumentation_asgi-0.45b0-py3-none-any.whl (14 kB)\n",
      "Collecting opentelemetry-instrumentation==0.45b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_instrumentation-0.45b0-py3-none-any.whl (28 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.45b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_semantic_conventions-0.45b0-py3-none-any.whl (36 kB)\n",
      "Collecting opentelemetry-util-http==0.45b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_util_http-0.45b0-py3-none-any.whl (6.9 kB)\n",
      "Requirement already satisfied: setuptools>=16.0 in c:\\users\\hossein rahmani\\pycharmprojects\\chatgpt_prompting\\.venv\\lib\\site-packages (from opentelemetry-instrumentation==0.45b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (58.1.0)\n",
      "Collecting wrapt<2.0.0,>=1.0.0 (from opentelemetry-instrumentation==0.45b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading wrapt-1.16.0-cp310-cp310-win_amd64.whl (37 kB)\n",
      "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.45b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
      "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
      "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\hossein rahmani\\pycharmprojects\\chatgpt_prompting\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in c:\\users\\hossein rahmani\\pycharmprojects\\chatgpt_prompting\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.16.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hossein rahmani\\pycharmprojects\\chatgpt_prompting\\.venv\\lib\\site-packages (from requests<3,>=2->Langchain) (3.3.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\hossein rahmani\\pycharmprojects\\chatgpt_prompting\\.venv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->Langchain) (3.0.3)\n",
      "Collecting huggingface_hub<1.0,>=0.16.4 (from tokenizers>=0.13.2->chromadb)\n",
      "  Downloading huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\n",
      "                                              0.0/388.9 kB ? eta -:--:--\n",
      "     ------------------------------------- 388.9/388.9 kB 12.2 MB/s eta 0:00:00\n",
      "Collecting typer-slim[standard]==0.12.0 (from typer>=0.9.0->chromadb)\n",
      "  Downloading typer_slim-0.12.0-py3-none-any.whl (46 kB)\n",
      "                                              0.0/46.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 46.8/46.8 kB 2.3 MB/s eta 0:00:00\n",
      "Collecting typer-cli==0.12.0 (from typer>=0.9.0->chromadb)\n",
      "  Downloading typer_cli-0.12.0-py3-none-any.whl (3.0 kB)\n",
      "Collecting click>=8.0.0 (from typer-slim[standard]==0.12.0->typer>=0.9.0->chromadb)\n",
      "  Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Collecting shellingham>=1.3.0 (from typer-slim[standard]==0.12.0->typer>=0.9.0->chromadb)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Collecting rich>=10.11.0 (from typer-slim[standard]==0.12.0->typer>=0.9.0->chromadb)\n",
      "  Downloading rich-13.7.1-py3-none-any.whl (240 kB)\n",
      "                                              0.0/240.7 kB ? eta -:--:--\n",
      "     ------------------------------------  235.5/240.7 kB 14.1 MB/s eta 0:00:01\n",
      "     -------------------------------------- 240.7/240.7 kB 4.9 MB/s eta 0:00:00\n",
      "Collecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading httptools-0.6.1-cp310-cp310-win_amd64.whl (58 kB)\n",
      "                                              0.0/58.2 kB ? eta -:--:--\n",
      "     ---------------------------------------- 58.2/58.2 kB 1.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: python-dotenv>=0.13 in c:\\users\\hossein rahmani\\pycharmprojects\\chatgpt_prompting\\.venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.1)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading watchfiles-0.21.0-cp310-none-win_amd64.whl (279 kB)\n",
      "                                              0.0/279.7 kB ? eta -:--:--\n",
      "     -------------------------------------- 279.7/279.7 kB 8.4 MB/s eta 0:00:00\n",
      "Collecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading websockets-12.0-cp310-cp310-win_amd64.whl (124 kB)\n",
      "                                              0.0/125.0 kB ? eta -:--:--\n",
      "     -------------------------------------- 125.0/125.0 kB 7.2 MB/s eta 0:00:00\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Downloading cachetools-5.3.3-py3-none-any.whl (9.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Downloading pyasn1_modules-0.4.0-py3-none-any.whl (181 kB)\n",
      "                                              0.0/181.2 kB ? eta -:--:--\n",
      "     -------------------------------------- 181.2/181.2 kB 5.5 MB/s eta 0:00:00\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting filelock (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb)\n",
      "  Downloading filelock-3.13.3-py3-none-any.whl (11 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb)\n",
      "  Downloading fsspec-2024.3.1-py3-none-any.whl (171 kB)\n",
      "                                              0.0/172.0 kB ? eta -:--:--\n",
      "     ------------------------------------- 172.0/172.0 kB 10.1 MB/s eta 0:00:00\n",
      "Collecting zipp>=0.5 (from importlib-metadata>=4.6->build>=1.0.3->chromadb)\n",
      "  Downloading zipp-3.18.1-py3-none-any.whl (8.2 kB)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\hossein rahmani\\pycharmprojects\\chatgpt_prompting\\.venv\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->Langchain) (1.0.0)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "                                              0.0/86.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 86.8/86.8 kB 2.5 MB/s eta 0:00:00\n",
      "Collecting mpmath>=0.19 (from sympy->onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "                                              0.0/536.2 kB ? eta -:--:--\n",
      "     ------------------------------------- 536.2/536.2 kB 11.2 MB/s eta 0:00:00\n",
      "Collecting pyreadline3 (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading pyreadline3-3.4.1-py3-none-any.whl (95 kB)\n",
      "                                              0.0/95.2 kB ? eta -:--:--\n",
      "     ---------------------------------------- 95.2/95.2 kB 5.3 MB/s eta 0:00:00\n",
      "Collecting pyasn1<0.7.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Downloading pyasn1-0.6.0-py2.py3-none-any.whl (85 kB)\n",
      "                                              0.0/85.3 kB ? eta -:--:--\n",
      "     ---------------------------------------- 85.3/85.3 kB 5.0 MB/s eta 0:00:00\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer-slim[standard]==0.12.0->typer>=0.9.0->chromadb)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "                                              0.0/87.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 87.5/87.5 kB 4.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\hossein rahmani\\pycharmprojects\\chatgpt_prompting\\.venv\\lib\\site-packages (from rich>=10.11.0->typer-slim[standard]==0.12.0->typer>=0.9.0->chromadb) (2.15.1)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer-slim[standard]==0.12.0->typer>=0.9.0->chromadb)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Building wheels for collected packages: pypika\n",
      "  Building wheel for pypika (pyproject.toml): started\n",
      "  Building wheel for pypika (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53835 sha256=26f4ffe6800cbca75f1b48641eba912e7945ddb6fc85c92b199960d7b6ed7de5\n",
      "  Stored in directory: c:\\users\\hossein rahmani\\appdata\\local\\pip\\cache\\wheels\\e1\\26\\51\\d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
      "Successfully built pypika\n",
      "Installing collected packages: pyreadline3, pypika, mpmath, monotonic, mmh3, flatbuffers, zipp, wrapt, websockets, websocket-client, tomli, sympy, shellingham, pyasn1, pulsar-client, protobuf, overrides, opentelemetry-util-http, opentelemetry-semantic-conventions, oauthlib, mdurl, importlib-resources, humanfriendly, httptools, grpcio, fsspec, filelock, click, chroma-hnswlib, cachetools, bcrypt, backoff, asgiref, watchfiles, uvicorn, typer-slim, starlette, rsa, requests-oauthlib, pyproject_hooks, pyasn1-modules, posthog, opentelemetry-proto, markdown-it-py, importlib-metadata, huggingface_hub, googleapis-common-protos, deprecated, coloredlogs, tokenizers, rich, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, onnxruntime, google-auth, fastapi, build, opentelemetry-sdk, opentelemetry-instrumentation, kubernetes, typer-cli, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, typer, opentelemetry-instrumentation-fastapi, chromadb\n",
      "Successfully installed asgiref-3.8.1 backoff-2.2.1 bcrypt-4.1.2 build-1.2.1 cachetools-5.3.3 chroma-hnswlib-0.7.3 chromadb-0.4.24 click-8.1.7 coloredlogs-15.0.1 deprecated-1.2.14 fastapi-0.110.0 filelock-3.13.3 flatbuffers-24.3.25 fsspec-2024.3.1 google-auth-2.29.0 googleapis-common-protos-1.63.0 grpcio-1.62.1 httptools-0.6.1 huggingface_hub-0.22.2 humanfriendly-10.0 importlib-metadata-7.0.0 importlib-resources-6.4.0 kubernetes-29.0.0 markdown-it-py-3.0.0 mdurl-0.1.2 mmh3-4.1.0 monotonic-1.6 mpmath-1.3.0 oauthlib-3.2.2 onnxruntime-1.17.1 opentelemetry-api-1.24.0 opentelemetry-exporter-otlp-proto-common-1.24.0 opentelemetry-exporter-otlp-proto-grpc-1.24.0 opentelemetry-instrumentation-0.45b0 opentelemetry-instrumentation-asgi-0.45b0 opentelemetry-instrumentation-fastapi-0.45b0 opentelemetry-proto-1.24.0 opentelemetry-sdk-1.24.0 opentelemetry-semantic-conventions-0.45b0 opentelemetry-util-http-0.45b0 overrides-7.7.0 posthog-3.5.0 protobuf-4.25.3 pulsar-client-3.4.0 pyasn1-0.6.0 pyasn1-modules-0.4.0 pypika-0.48.9 pyproject_hooks-1.0.0 pyreadline3-3.4.1 requests-oauthlib-2.0.0 rich-13.7.1 rsa-4.9 shellingham-1.5.4 starlette-0.36.3 sympy-1.12 tokenizers-0.15.2 tomli-2.0.1 typer-0.12.0 typer-cli-0.12.0 typer-slim-0.12.0 uvicorn-0.29.0 watchfiles-0.21.0 websocket-client-1.7.0 websockets-12.0 wrapt-1.16.0 zipp-3.18.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# installing necassary things\n",
    "! pip install openai pypdf Langchain chromadb sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bitcoin: A Peer-to-Peer Electronic Cash System\n",
      "Satoshi\n",
      "Nakamoto\n",
      "satoshin@gmx.com\n",
      "www.bitcoin.org\n",
      "Abstract.  A purely\n",
      "peer-to-peer version of electronic cash would allow online  \n",
      "payments\n",
      "to be sent directly from one party to another without going through a \n",
      "\n",
      "financial institution.  Digital signatures provide part of the\n",
      "solution, but the main  \n",
      "benefits are lost if a trusted third party is\n",
      "still required to prevent double-spending.  \n",
      "We propose a solution to\n",
      "the double-spending problem using a peer-to-peer network.  \n",
      "The network\n",
      "timestamps transactions by hashing them into an ongoing chain of \n",
      "\n",
      "hash-based proof-of-work, forming a record that cannot be changed\n",
      "without redoing  \n",
      "the proof-of-work.  The longest chain not only serves\n",
      "as proof of the sequence of  \n",
      "events witnessed, but proof that it came\n",
      "from the largest pool of CPU power.  As  \n",
      "long as a majority of CPU\n",
      "power is controlled by nodes that are not cooperating to  \n",
      "attack the\n",
      "network, they'll generate the longest chain and outpace attackers.  The\n",
      " \n",
      "network itself requires minimal structure.  Messages are broadcast on\n",
      "a best effort  \n",
      "basis, and nodes can leave and rejoin the network at\n",
      "will, accepting the longest  \n",
      "proof-of-work chain as proof of what\n",
      "happened while they were gone.\n",
      "1.Introduction\n",
      "Commerce on the Internet\n",
      "has come to rely almost exclusively on financial institutions serving\n",
      "as  \n",
      "trusted third parties to process electronic payments.  While the\n",
      "system works well enough for  \n",
      "most  transactions,  it  still  suffers \n",
      "from  the  inherent  weaknesses  of  the  trust  based  model. \n",
      "\n",
      "Completely non-reversible transactions are not really possible, since\n",
      "financial institutions cannot  \n",
      "avoid  mediating  disputes.   The  cost\n",
      " of  mediation  increases  transaction  costs,  limiting  the  \n",
      "minimum\n",
      "practical transaction size and cutting off the possibility for small\n",
      "casual transactions,  \n",
      "and there is a broader cost in the loss of\n",
      "ability to make non-reversible payments for non-\n",
      "reversible services. \n",
      "With the possibility of reversal, the need for trust spreads. \n",
      "Merchants must  \n",
      "be wary of their customers, hassling them for more\n",
      "information than they would otherwise need.  \n",
      "A certain percentage of\n",
      "fraud is accepted as unavoidable.  These costs and payment\n",
      "uncertainties  \n",
      "can be avoided in person by using physical currency,\n",
      "but no mechanism exists to make payments  \n",
      "over a communications\n",
      "channel without a trusted party.\n",
      "What is needed is an electronic\n",
      "payment system based on cryptographic proof instead of trust, \n",
      "\n",
      "allowing any two willing parties to transact directly with each other\n",
      "without the need for a trusted  \n",
      "third party.  Transactions that are\n",
      "computationally impractical to reverse would protect sellers  \n",
      "from\n",
      "fraud, and routine escrow mechanisms could easily be implemented to\n",
      "protect buyers.  In  \n",
      "this paper, we propose a solution to the\n",
      "double-spending problem using a peer-to-peer distributed  \n",
      "timestamp\n",
      "server to generate computational proof of the chronological order of\n",
      "transactions.  The  \n",
      "system  is  secure  as  long  as  honest  nodes \n",
      "collectively  control  more  CPU  power  than  any  \n",
      "cooperating group\n",
      "of attacker nodes.\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "def word_wrap(string, n_chars=72):\n",
    "    # Wrap a string at the next space after n_chars\n",
    "    if len(string) < n_chars:\n",
    "        return string\n",
    "    else:\n",
    "        return string[:n_chars].rsplit(' ', 1)[0] + '\\n' + word_wrap(string[len(string[:n_chars].rsplit(' ', 1)[0])+1:], n_chars)\n",
    "\n",
    "from pypdf import PdfReader\n",
    "\n",
    "reader = PdfReader(\"bitcoin-Satoshi.pdf\")\n",
    "pdf_texts = [p.extract_text().strip() for p in reader.pages]\n",
    "\n",
    "# Filter the empty strings\n",
    "pdf_texts = [text for text in pdf_texts if text]\n",
    "\n",
    "print(word_wrap(pdf_texts[0])) # just to check if it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "but save the other branch in case it becomes longer.  The tie will be\n",
      "broken when the next proof-\n",
      "of-work is found and one branch becomes\n",
      "longer; the nodes that were working on the other  \n",
      "branch will then\n",
      "switch to the longer one.\n",
      "3Block\n",
      "Prev HashNonce\n",
      "TxTx...Block\n",
      "Prev\n",
      "HashNonce\n",
      "TxTx...\n",
      "\n",
      "Total chunks: 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hossein rahmani\\PycharmProjects\\CHATGPT_Prompting\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\hossein rahmani\\PycharmProjects\\CHATGPT_Prompting\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:148: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\hossein rahmani\\.cache\\huggingface\\hub\\models--sentence-transformers--all-mpnet-base-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "but save the other branch in case it becomes longer. the tie will be\n",
      "broken when the next proof - of - work is found and one branch becomes\n",
      "longer ; the nodes that were working on the other branch will then\n",
      "switch to the longer one. 3block prev hashnonce txtx... block prev\n",
      "hashnonce txtx...\n",
      "\n",
      "Total chunks: 29\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, SentenceTransformersTokenTextSplitter\n",
    "\n",
    "character_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=0\n",
    ")\n",
    "character_split_texts = character_splitter.split_text('\\n\\n'.join(pdf_texts))\n",
    "\n",
    "print(word_wrap(character_split_texts[10]))\n",
    "print(f\"\\nTotal chunks: {len(character_split_texts)}\")\n",
    "\n",
    "token_splitter = SentenceTransformersTokenTextSplitter(chunk_overlap=0, tokens_per_chunk=256)\n",
    "\n",
    "token_split_texts = []\n",
    "for text in character_split_texts:\n",
    "    token_split_texts += token_splitter.split_text(text)\n",
    "\n",
    "print(word_wrap(token_split_texts[10]))\n",
    "print(f\"\\nTotal chunks: {len(token_split_texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence_transformers\n",
      "  Downloading sentence_transformers-2.6.1-py3-none-any.whl (163 kB)\n",
      "                                              0.0/163.3 kB ? eta -:--:--\n",
      "     -----------------------------------    153.6/163.3 kB 9.6 MB/s eta 0:00:01\n",
      "     -------------------------------------- 163.3/163.3 kB 2.0 MB/s eta 0:00:00\n",
      "Collecting transformers<5.0.0,>=4.32.0 (from sentence_transformers)\n",
      "  Downloading transformers-4.39.2-py3-none-any.whl (8.8 MB)\n",
      "                                              0.0/8.8 MB ? eta -:--:--\n",
      "     ----                                     0.9/8.8 MB 19.6 MB/s eta 0:00:01\n",
      "     ----------                               2.2/8.8 MB 20.1 MB/s eta 0:00:01\n",
      "     -------------                            3.0/8.8 MB 19.0 MB/s eta 0:00:01\n",
      "     -----------------                        4.0/8.8 MB 19.4 MB/s eta 0:00:01\n",
      "     -----------------------                  5.3/8.8 MB 19.9 MB/s eta 0:00:01\n",
      "     ------------------------------           6.6/8.8 MB 21.1 MB/s eta 0:00:01\n",
      "     -----------------------------------      7.9/8.8 MB 21.0 MB/s eta 0:00:01\n",
      "     --------------------------------------   8.5/8.8 MB 21.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------  8.8/8.8 MB 20.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  8.8/8.8 MB 20.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  8.8/8.8 MB 20.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 8.8/8.8 MB 15.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm in c:\\users\\hossein rahmani\\pycharmprojects\\chatgpt_prompting\\.venv\\lib\\site-packages (from sentence_transformers) (4.66.1)\n",
      "Collecting torch>=1.11.0 (from sentence_transformers)\n",
      "  Downloading torch-2.2.2-cp310-cp310-win_amd64.whl (198.6 MB)\n",
      "                                              0.0/198.6 MB ? eta -:--:--\n",
      "                                             1.2/198.6 MB 39.3 MB/s eta 0:00:06\n",
      "                                             2.5/198.6 MB 26.7 MB/s eta 0:00:08\n",
      "                                             3.5/198.6 MB 24.9 MB/s eta 0:00:08\n",
      "                                             4.9/198.6 MB 24.3 MB/s eta 0:00:08\n",
      "     -                                       5.9/198.6 MB 23.5 MB/s eta 0:00:09\n",
      "     -                                       7.0/198.6 MB 22.5 MB/s eta 0:00:09\n",
      "     -                                       8.0/198.6 MB 22.3 MB/s eta 0:00:09\n",
      "     -                                       9.5/198.6 MB 22.5 MB/s eta 0:00:09\n",
      "     -                                      10.3/198.6 MB 21.8 MB/s eta 0:00:09\n",
      "     --                                     11.7/198.6 MB 21.8 MB/s eta 0:00:09\n",
      "     --                                     12.5/198.6 MB 21.1 MB/s eta 0:00:09\n",
      "     --                                     13.4/198.6 MB 21.1 MB/s eta 0:00:09\n",
      "     --                                     14.5/198.6 MB 20.5 MB/s eta 0:00:09\n",
      "     --                                     15.6/198.6 MB 19.8 MB/s eta 0:00:10\n",
      "     ---                                    16.6/198.6 MB 19.3 MB/s eta 0:00:10\n",
      "     ---                                    17.5/198.6 MB 18.7 MB/s eta 0:00:10\n",
      "     ---                                    18.8/198.6 MB 18.7 MB/s eta 0:00:10\n",
      "     ---                                    19.9/198.6 MB 18.2 MB/s eta 0:00:10\n",
      "     ---                                    20.8/198.6 MB 18.2 MB/s eta 0:00:10\n",
      "     ----                                   22.3/198.6 MB 18.7 MB/s eta 0:00:10\n",
      "     ----                                   22.9/198.6 MB 18.2 MB/s eta 0:00:10\n",
      "     ----                                   23.6/198.6 MB 17.7 MB/s eta 0:00:10\n",
      "     ----                                   24.9/198.6 MB 18.2 MB/s eta 0:00:10\n",
      "     ----                                   26.0/198.6 MB 18.2 MB/s eta 0:00:10\n",
      "     -----                                  27.0/198.6 MB 18.7 MB/s eta 0:00:10\n",
      "     -----                                  28.1/198.6 MB 18.7 MB/s eta 0:00:10\n",
      "     -----                                  29.4/198.6 MB 18.7 MB/s eta 0:00:10\n",
      "     -----                                  30.5/198.6 MB 18.2 MB/s eta 0:00:10\n",
      "     ------                                 31.6/198.6 MB 18.2 MB/s eta 0:00:10\n",
      "     ------                                 32.8/198.6 MB 18.2 MB/s eta 0:00:10\n",
      "     ------                                 33.6/198.6 MB 18.7 MB/s eta 0:00:09\n",
      "     ------                                 34.7/198.6 MB 18.2 MB/s eta 0:00:10\n",
      "     ------                                 35.6/198.6 MB 18.2 MB/s eta 0:00:09\n",
      "     -------                                36.7/198.6 MB 18.2 MB/s eta 0:00:09\n",
      "     -------                                37.6/198.6 MB 18.7 MB/s eta 0:00:09\n",
      "     -------                                38.4/198.6 MB 19.3 MB/s eta 0:00:09\n",
      "     -------                                39.1/198.6 MB 18.2 MB/s eta 0:00:09\n",
      "     -------                                40.4/198.6 MB 18.2 MB/s eta 0:00:09\n",
      "     -------                                41.3/198.6 MB 18.7 MB/s eta 0:00:09\n",
      "     --------                               42.2/198.6 MB 18.7 MB/s eta 0:00:09\n",
      "     --------                               43.0/198.6 MB 18.2 MB/s eta 0:00:09\n",
      "     --------                               43.8/198.6 MB 18.7 MB/s eta 0:00:09\n",
      "     --------                               45.1/198.6 MB 18.7 MB/s eta 0:00:09\n",
      "     --------                               46.0/198.6 MB 19.2 MB/s eta 0:00:08\n",
      "     ---------                              47.3/198.6 MB 18.7 MB/s eta 0:00:09\n",
      "     ---------                              48.5/198.6 MB 18.7 MB/s eta 0:00:09\n",
      "     ---------                              49.5/198.6 MB 19.8 MB/s eta 0:00:08\n",
      "     ---------                              50.8/198.6 MB 19.8 MB/s eta 0:00:08\n",
      "     ---------                              51.8/198.6 MB 20.5 MB/s eta 0:00:08\n",
      "     ----------                             53.0/198.6 MB 20.5 MB/s eta 0:00:08\n",
      "     ----------                             54.2/198.6 MB 21.1 MB/s eta 0:00:07\n",
      "     ----------                             55.2/198.6 MB 20.5 MB/s eta 0:00:08\n",
      "     ----------                             56.5/198.6 MB 21.1 MB/s eta 0:00:07\n",
      "     ----------                             57.2/198.6 MB 20.5 MB/s eta 0:00:07\n",
      "     -----------                            58.2/198.6 MB 19.8 MB/s eta 0:00:08\n",
      "     -----------                            59.1/198.6 MB 19.8 MB/s eta 0:00:08\n",
      "     -----------                            60.2/198.6 MB 19.3 MB/s eta 0:00:08\n",
      "     -----------                            61.1/198.6 MB 19.9 MB/s eta 0:00:07\n",
      "     -----------                            62.1/198.6 MB 19.9 MB/s eta 0:00:07\n",
      "     ------------                           63.1/198.6 MB 18.7 MB/s eta 0:00:08\n",
      "     ------------                           64.1/198.6 MB 18.7 MB/s eta 0:00:08\n",
      "     ------------                           64.9/198.6 MB 18.7 MB/s eta 0:00:08\n",
      "     ------------                           66.1/198.6 MB 18.2 MB/s eta 0:00:08\n",
      "     ------------                           67.3/198.6 MB 18.2 MB/s eta 0:00:08\n",
      "     ------------                           67.9/198.6 MB 18.2 MB/s eta 0:00:08\n",
      "     -------------                          69.1/198.6 MB 18.2 MB/s eta 0:00:08\n",
      "     -------------                          71.3/198.6 MB 21.1 MB/s eta 0:00:07\n",
      "     --------------                         73.4/198.6 MB 26.2 MB/s eta 0:00:05\n",
      "     --------------                         74.5/198.6 MB 25.2 MB/s eta 0:00:05\n",
      "     --------------                         76.4/198.6 MB 29.7 MB/s eta 0:00:05\n",
      "     --------------                         76.5/198.6 MB 31.2 MB/s eta 0:00:04\n",
      "     --------------                         76.5/198.6 MB 31.2 MB/s eta 0:00:04\n",
      "     --------------                         76.5/198.6 MB 31.2 MB/s eta 0:00:04\n",
      "     --------------                         76.9/198.6 MB 19.8 MB/s eta 0:00:07\n",
      "     --------------                         77.9/198.6 MB 19.2 MB/s eta 0:00:07\n",
      "     ---------------                        78.4/198.6 MB 18.7 MB/s eta 0:00:07\n",
      "     ---------------                        79.2/198.6 MB 18.7 MB/s eta 0:00:07\n",
      "     ---------------                        80.1/198.6 MB 18.2 MB/s eta 0:00:07\n",
      "     ---------------                        81.2/198.6 MB 16.4 MB/s eta 0:00:08\n",
      "     ---------------                        82.4/198.6 MB 15.6 MB/s eta 0:00:08\n",
      "     ---------------                        83.4/198.6 MB 14.9 MB/s eta 0:00:08\n",
      "     ----------------                       84.3/198.6 MB 14.9 MB/s eta 0:00:08\n",
      "     ----------------                       85.1/198.6 MB 14.6 MB/s eta 0:00:08\n",
      "     ----------------                       86.4/198.6 MB 13.6 MB/s eta 0:00:09\n",
      "     ----------------                       87.2/198.6 MB 17.7 MB/s eta 0:00:07\n",
      "     ----------------                       88.6/198.6 MB 18.7 MB/s eta 0:00:06\n",
      "     -----------------                      89.9/198.6 MB 19.3 MB/s eta 0:00:06\n",
      "     -----------------                      91.0/198.6 MB 19.3 MB/s eta 0:00:06\n",
      "     -----------------                      91.9/198.6 MB 19.9 MB/s eta 0:00:06\n",
      "     -----------------                      93.0/198.6 MB 19.8 MB/s eta 0:00:06\n",
      "     -----------------                      93.9/198.6 MB 19.8 MB/s eta 0:00:06\n",
      "     ------------------                     94.8/198.6 MB 19.3 MB/s eta 0:00:06\n",
      "     ------------------                     95.5/198.6 MB 19.3 MB/s eta 0:00:06\n",
      "     ------------------                     96.8/198.6 MB 19.8 MB/s eta 0:00:06\n",
      "     ------------------                     98.2/198.6 MB 19.9 MB/s eta 0:00:06\n",
      "     ------------------                     99.1/198.6 MB 19.9 MB/s eta 0:00:06\n",
      "     -------------------                    99.9/198.6 MB 19.3 MB/s eta 0:00:06\n",
      "     ------------------                    101.1/198.6 MB 18.7 MB/s eta 0:00:06\n",
      "     -------------------                   102.0/198.6 MB 18.7 MB/s eta 0:00:06\n",
      "     -------------------                   103.1/198.6 MB 18.7 MB/s eta 0:00:06\n",
      "     -------------------                   104.2/198.6 MB 19.3 MB/s eta 0:00:05\n",
      "     -------------------                   105.4/198.6 MB 19.3 MB/s eta 0:00:05\n",
      "     -------------------                   106.4/198.6 MB 19.3 MB/s eta 0:00:05\n",
      "     -------------------                   107.2/198.6 MB 18.7 MB/s eta 0:00:05\n",
      "     --------------------                  108.1/198.6 MB 18.2 MB/s eta 0:00:05\n",
      "     --------------------                  109.0/198.6 MB 17.7 MB/s eta 0:00:06\n",
      "     --------------------                  109.9/198.6 MB 17.2 MB/s eta 0:00:06\n",
      "     --------------------                  110.1/198.6 MB 16.0 MB/s eta 0:00:06\n",
      "     --------------------                  111.0/198.6 MB 16.4 MB/s eta 0:00:06\n",
      "     --------------------                  112.4/198.6 MB 16.8 MB/s eta 0:00:06\n",
      "     ---------------------                 113.6/198.6 MB 16.4 MB/s eta 0:00:06\n",
      "     ---------------------                 114.4/198.6 MB 16.0 MB/s eta 0:00:06\n",
      "     ---------------------                 115.3/198.6 MB 16.4 MB/s eta 0:00:06\n",
      "     ---------------------                 116.2/198.6 MB 16.4 MB/s eta 0:00:06\n",
      "     ---------------------                 117.3/198.6 MB 16.4 MB/s eta 0:00:05\n",
      "     ----------------------                118.3/198.6 MB 16.8 MB/s eta 0:00:05\n",
      "     ----------------------                119.1/198.6 MB 16.8 MB/s eta 0:00:05\n",
      "     ----------------------                120.0/198.6 MB 16.8 MB/s eta 0:00:05\n",
      "     ----------------------                120.9/198.6 MB 18.7 MB/s eta 0:00:05\n",
      "     ----------------------                121.5/198.6 MB 18.2 MB/s eta 0:00:05\n",
      "     ----------------------                122.2/198.6 MB 17.7 MB/s eta 0:00:05\n",
      "     ----------------------                122.9/198.6 MB 17.7 MB/s eta 0:00:05\n",
      "     -----------------------               124.0/198.6 MB 17.7 MB/s eta 0:00:05\n",
      "     -----------------------               125.2/198.6 MB 17.7 MB/s eta 0:00:05\n",
      "     -----------------------               126.3/198.6 MB 17.7 MB/s eta 0:00:05\n",
      "     -----------------------               127.2/198.6 MB 17.7 MB/s eta 0:00:05\n",
      "     -----------------------               128.5/198.6 MB 18.2 MB/s eta 0:00:04\n",
      "     ------------------------              129.8/198.6 MB 18.7 MB/s eta 0:00:04\n",
      "     ------------------------              130.9/198.6 MB 18.7 MB/s eta 0:00:04\n",
      "     ------------------------              131.9/198.6 MB 19.2 MB/s eta 0:00:04\n",
      "     ------------------------              133.1/198.6 MB 19.8 MB/s eta 0:00:04\n",
      "     -------------------------             134.5/198.6 MB 20.5 MB/s eta 0:00:04\n",
      "     -------------------------             135.4/198.6 MB 20.5 MB/s eta 0:00:04\n",
      "     -------------------------             136.6/198.6 MB 21.1 MB/s eta 0:00:03\n",
      "     -------------------------             137.6/198.6 MB 21.1 MB/s eta 0:00:03\n",
      "     -------------------------             138.7/198.6 MB 21.1 MB/s eta 0:00:03\n",
      "     --------------------------            139.8/198.6 MB 20.5 MB/s eta 0:00:03\n",
      "     --------------------------            140.7/198.6 MB 20.5 MB/s eta 0:00:03\n",
      "     --------------------------            142.1/198.6 MB 20.5 MB/s eta 0:00:03\n",
      "     --------------------------            143.0/198.6 MB 21.1 MB/s eta 0:00:03\n",
      "     --------------------------            144.1/198.6 MB 21.1 MB/s eta 0:00:03\n",
      "     ---------------------------           145.5/198.6 MB 21.1 MB/s eta 0:00:03\n",
      "     ---------------------------           146.4/198.6 MB 21.1 MB/s eta 0:00:03\n",
      "     ---------------------------           147.2/198.6 MB 21.1 MB/s eta 0:00:03\n",
      "     ---------------------------           148.1/198.6 MB 21.1 MB/s eta 0:00:03\n",
      "     ---------------------------           149.4/198.6 MB 21.1 MB/s eta 0:00:03\n",
      "     ----------------------------          150.5/198.6 MB 21.1 MB/s eta 0:00:03\n",
      "     ----------------------------          151.4/198.6 MB 21.1 MB/s eta 0:00:03\n",
      "     ----------------------------          152.2/198.6 MB 19.8 MB/s eta 0:00:03\n",
      "     ----------------------------          153.2/198.6 MB 19.3 MB/s eta 0:00:03\n",
      "     ----------------------------          154.4/198.6 MB 19.8 MB/s eta 0:00:03\n",
      "     ----------------------------          155.3/198.6 MB 19.3 MB/s eta 0:00:03\n",
      "     -----------------------------         156.3/198.6 MB 19.3 MB/s eta 0:00:03\n",
      "     -----------------------------         157.5/198.6 MB 19.3 MB/s eta 0:00:03\n",
      "     -----------------------------         158.6/198.6 MB 19.8 MB/s eta 0:00:03\n",
      "     -----------------------------         159.5/198.6 MB 19.2 MB/s eta 0:00:03\n",
      "     -----------------------------         160.7/198.6 MB 18.7 MB/s eta 0:00:03\n",
      "     ------------------------------        161.9/198.6 MB 19.2 MB/s eta 0:00:02\n",
      "     ------------------------------        162.7/198.6 MB 19.9 MB/s eta 0:00:02\n",
      "     ------------------------------        163.6/198.6 MB 19.3 MB/s eta 0:00:02\n",
      "     ------------------------------        164.5/198.6 MB 18.2 MB/s eta 0:00:02\n",
      "     ------------------------------        165.8/198.6 MB 18.7 MB/s eta 0:00:02\n",
      "     -------------------------------       166.7/198.6 MB 18.7 MB/s eta 0:00:02\n",
      "     -------------------------------       167.6/198.6 MB 18.2 MB/s eta 0:00:02\n",
      "     -------------------------------       168.3/198.6 MB 18.2 MB/s eta 0:00:02\n",
      "     -------------------------------       169.0/198.6 MB 18.2 MB/s eta 0:00:02\n",
      "     -------------------------------       170.3/198.6 MB 18.2 MB/s eta 0:00:02\n",
      "     -------------------------------       171.0/198.6 MB 18.2 MB/s eta 0:00:02\n",
      "     --------------------------------      172.2/198.6 MB 17.7 MB/s eta 0:00:02\n",
      "     --------------------------------      173.0/198.6 MB 18.2 MB/s eta 0:00:02\n",
      "     --------------------------------      174.1/198.6 MB 18.7 MB/s eta 0:00:02\n",
      "     --------------------------------      175.3/198.6 MB 18.2 MB/s eta 0:00:02\n",
      "     --------------------------------      176.4/198.6 MB 18.2 MB/s eta 0:00:02\n",
      "     ---------------------------------     177.5/198.6 MB 17.7 MB/s eta 0:00:02\n",
      "     ---------------------------------     178.3/198.6 MB 17.7 MB/s eta 0:00:02\n",
      "     ---------------------------------     179.5/198.6 MB 18.2 MB/s eta 0:00:02\n",
      "     ---------------------------------     180.6/198.6 MB 18.2 MB/s eta 0:00:01\n",
      "     ---------------------------------     181.7/198.6 MB 18.7 MB/s eta 0:00:01\n",
      "     ----------------------------------    182.6/198.6 MB 18.2 MB/s eta 0:00:01\n",
      "     ----------------------------------    183.8/198.6 MB 18.2 MB/s eta 0:00:01\n",
      "     ----------------------------------    185.1/198.6 MB 18.7 MB/s eta 0:00:01\n",
      "     ----------------------------------    186.2/198.6 MB 19.3 MB/s eta 0:00:01\n",
      "     ----------------------------------    187.5/198.6 MB 19.3 MB/s eta 0:00:01\n",
      "     -----------------------------------   188.3/198.6 MB 19.2 MB/s eta 0:00:01\n",
      "     -----------------------------------   189.4/198.6 MB 19.8 MB/s eta 0:00:01\n",
      "     -----------------------------------   190.7/198.6 MB 19.8 MB/s eta 0:00:01\n",
      "     -----------------------------------   191.8/198.6 MB 19.8 MB/s eta 0:00:01\n",
      "     -----------------------------------   192.8/198.6 MB 20.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  193.7/198.6 MB 21.1 MB/s eta 0:00:01\n",
      "     ------------------------------------  194.7/198.6 MB 21.1 MB/s eta 0:00:01\n",
      "     ------------------------------------  195.5/198.6 MB 21.1 MB/s eta 0:00:01\n",
      "     ------------------------------------  196.6/198.6 MB 21.1 MB/s eta 0:00:01\n",
      "     ------------------------------------  197.6/198.6 MB 19.9 MB/s eta 0:00:01\n",
      "     ------------------------------------  198.6/198.6 MB 20.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  198.6/198.6 MB 20.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  198.6/198.6 MB 20.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  198.6/198.6 MB 20.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  198.6/198.6 MB 20.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  198.6/198.6 MB 20.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  198.6/198.6 MB 20.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  198.6/198.6 MB 20.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  198.6/198.6 MB 20.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  198.6/198.6 MB 20.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  198.6/198.6 MB 20.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  198.6/198.6 MB 20.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  198.6/198.6 MB 20.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  198.6/198.6 MB 20.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  198.6/198.6 MB 20.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  198.6/198.6 MB 20.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  198.6/198.6 MB 20.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  198.6/198.6 MB 20.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  198.6/198.6 MB 20.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  198.6/198.6 MB 20.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  198.6/198.6 MB 20.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  198.6/198.6 MB 20.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  198.6/198.6 MB 20.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  198.6/198.6 MB 20.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  198.6/198.6 MB 20.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  198.6/198.6 MB 20.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  198.6/198.6 MB 20.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  198.6/198.6 MB 20.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  198.6/198.6 MB 20.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  198.6/198.6 MB 20.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  198.6/198.6 MB 20.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  198.6/198.6 MB 20.5 MB/s eta 0:00:01\n",
      "     -------------------------------------- 198.6/198.6 MB 3.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy in c:\\users\\hossein rahmani\\pycharmprojects\\chatgpt_prompting\\.venv\\lib\\site-packages (from sentence_transformers) (1.26.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\hossein rahmani\\pycharmprojects\\chatgpt_prompting\\.venv\\lib\\site-packages (from sentence_transformers) (1.3.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\hossein rahmani\\pycharmprojects\\chatgpt_prompting\\.venv\\lib\\site-packages (from sentence_transformers) (1.11.4)\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in c:\\users\\hossein rahmani\\pycharmprojects\\chatgpt_prompting\\.venv\\lib\\site-packages (from sentence_transformers) (0.22.2)\n",
      "Requirement already satisfied: Pillow in c:\\users\\hossein rahmani\\pycharmprojects\\chatgpt_prompting\\.venv\\lib\\site-packages (from sentence_transformers) (10.2.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\hossein rahmani\\pycharmprojects\\chatgpt_prompting\\.venv\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (3.13.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\hossein rahmani\\pycharmprojects\\chatgpt_prompting\\.venv\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2024.3.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\hossein rahmani\\pycharmprojects\\chatgpt_prompting\\.venv\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\hossein rahmani\\pycharmprojects\\chatgpt_prompting\\.venv\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\hossein rahmani\\pycharmprojects\\chatgpt_prompting\\.venv\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\hossein rahmani\\pycharmprojects\\chatgpt_prompting\\.venv\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (4.10.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\hossein rahmani\\pycharmprojects\\chatgpt_prompting\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\hossein rahmani\\pycharmprojects\\chatgpt_prompting\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (3.2.1)\n",
      "Collecting jinja2 (from torch>=1.11.0->sentence_transformers)\n",
      "  Using cached Jinja2-3.1.3-py3-none-any.whl (133 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\hossein rahmani\\pycharmprojects\\chatgpt_prompting\\.venv\\lib\\site-packages (from tqdm->sentence_transformers) (0.4.6)\n",
      "Collecting regex!=2019.12.17 (from transformers<5.0.0,>=4.32.0->sentence_transformers)\n",
      "  Downloading regex-2023.12.25-cp310-cp310-win_amd64.whl (269 kB)\n",
      "                                              0.0/269.5 kB ? eta -:--:--\n",
      "     -------------------------------------- 269.5/269.5 kB 8.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\hossein rahmani\\pycharmprojects\\chatgpt_prompting\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (0.15.2)\n",
      "Collecting safetensors>=0.4.1 (from transformers<5.0.0,>=4.32.0->sentence_transformers)\n",
      "  Downloading safetensors-0.4.2-cp310-none-win_amd64.whl (269 kB)\n",
      "                                              0.0/269.5 kB ? eta -:--:--\n",
      "     ---------------------------------------  266.2/269.5 kB ? eta -:--:--\n",
      "     -------------------------------------- 269.5/269.5 kB 8.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\hossein rahmani\\pycharmprojects\\chatgpt_prompting\\.venv\\lib\\site-packages (from scikit-learn->sentence_transformers) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\hossein rahmani\\pycharmprojects\\chatgpt_prompting\\.venv\\lib\\site-packages (from scikit-learn->sentence_transformers) (3.2.0)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch>=1.11.0->sentence_transformers)\n",
      "  Downloading MarkupSafe-2.1.5-cp310-cp310-win_amd64.whl (17 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hossein rahmani\\pycharmprojects\\chatgpt_prompting\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hossein rahmani\\pycharmprojects\\chatgpt_prompting\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hossein rahmani\\pycharmprojects\\chatgpt_prompting\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hossein rahmani\\pycharmprojects\\chatgpt_prompting\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\hossein rahmani\\pycharmprojects\\chatgpt_prompting\\.venv\\lib\\site-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Installing collected packages: safetensors, regex, MarkupSafe, jinja2, torch, transformers, sentence_transformers\n",
      "Successfully installed MarkupSafe-2.1.5 jinja2-3.1.3 regex-2023.12.25 safetensors-0.4.2 sentence_transformers-2.6.1 torch-2.2.2 transformers-4.39.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hossein rahmani\\PycharmProjects\\CHATGPT_Prompting\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:148: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\hossein rahmani\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import chromadb\n",
    "from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(path=\"bitcoin-Satoshi\")\n",
    "# chroma_client = chromadb.Client()\n",
    "emb_fn = SentenceTransformerEmbeddingFunction()\n",
    "chroma_collection = chroma_client.create_collection(\"bitcoin-Satoshi\", embedding_function=emb_fn)\n",
    "\n",
    "ids = [str(i) for i in range(len(token_split_texts))]\n",
    "\n",
    "chroma_collection.add(ids=ids, documents=token_split_texts)\n",
    "chroma_collection.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. proof - of - work to implement a distributed timestamp server on a\n",
      "peer - to - peer basis, we will need to use a proof - of - work system\n",
      "similar to adam back's hashcash [ 6 ], rather than newspaper or usenet\n",
      "posts. the proof - of - work involves scanning for a value that when\n",
      "hashed, such as with sha - 256, the hash begins with a number of zero\n",
      "bits. the average work required is exponential in the number of zero\n",
      "bits required and can be verified by executing a single hash. for our\n",
      "timestamp network, we implement the proof - of - work by incrementing a\n",
      "nonce in the block until a value is found that gives the block's hash\n",
      "the required zero bits. once the cpu effort has been expended to make\n",
      "it satisfy the proof - of - work, the block cannot be changed without\n",
      "redoing the work. as later blocks are chained after it, the work to\n",
      "change the block would include redoing all the blocks after it. the\n",
      "proof - of - work also solves the problem of determining representation\n",
      "in majority decision\n",
      "\n",
      "\n",
      "proposed a peer - to - peer network using proof - of - work to record a\n",
      "public history of transactions that quickly becomes computationally\n",
      "impractical for an attacker to change if honest nodes control a\n",
      "majority of cpu power. the network is robust in its unstructured\n",
      "simplicity. nodes work all at once with little coordination. they do\n",
      "not need to be identified, since messages are not routed to any\n",
      "particular place and only need to be delivered on a best effort basis.\n",
      "nodes can leave and rejoin the network at will, accepting the proof -\n",
      "of - work chain as proof of what happened while they were gone. they\n",
      "vote with their cpu power, expressing their acceptance of valid blocks\n",
      "by working on extending them and rejecting invalid blocks by refusing\n",
      "to work on them. any needed rules and incentives can be enforced with\n",
      "this consensus mechanism. 8\n",
      "\n",
      "\n",
      "8. simplified payment verification it is possible to verify payments\n",
      "without running a full network node. a user only needs to keep a copy\n",
      "of the block headers of the longest proof - of - work chain, which he\n",
      "can get by querying network nodes until he's convinced he has the\n",
      "longest chain, and obtain the merkle branch linking the transaction to\n",
      "the block it's timestamped in. he can't check the transaction for\n",
      "himself, but by linking it to a place in the chain, he can see that a\n",
      "network node has accepted it, and blocks added after it further confirm\n",
      "the network has accepted it. as such, the verification is reliable as\n",
      "long as honest nodes control the network, but is more vulnerable if the\n",
      "network is overpowered by an attacker. while network nodes can verify\n",
      "transactions for themselves, the simplified method can be fooled by an\n",
      "attacker's fabricated transactions for as long as the attacker can\n",
      "continue to overpower the network. one strategy to\n",
      "\n",
      "\n",
      "blocks per hour. if they're generated too fast, the difficulty\n",
      "increases. 5. network the steps to run the network are as follows : 1 )\n",
      "new transactions are broadcast to all nodes. 2 ) each node collects new\n",
      "transactions into a block. 3 ) each node works on finding a difficult\n",
      "proof - of - work for its block. 4 ) when a node finds a proof - of -\n",
      "work, it broadcasts the block to all nodes. 5 ) nodes accept the block\n",
      "only if all transactions in it are valid and not already spent. 6 )\n",
      "nodes express their acceptance of the block by working on creating the\n",
      "next block in the chain, using the hash of the accepted block as the\n",
      "previous hash. nodes always consider the longest chain to be the\n",
      "correct one and will keep working on extending it. if two nodes\n",
      "broadcast different versions of the next block simultaneously, some\n",
      "nodes may receive one or the other first. in that case, they work on\n",
      "the first one they received,\n",
      "\n",
      "\n",
      "making. if the majority were based on one - ip - address - one - vote,\n",
      "it could be subverted by anyone able to allocate many ips. proof - of -\n",
      "work is essentially one - cpu - one - vote. the majority decision is\n",
      "represented by the longest chain, which has the greatest proof - of -\n",
      "work effort invested in it. if a majority of cpu power is controlled by\n",
      "honest nodes, the honest chain will grow the fastest and outpace any\n",
      "competing chains. to modify a past block, an attacker would have to\n",
      "redo the proof - of - work of the block and all blocks after it and\n",
      "then catch up with and surpass the work of the honest nodes. we will\n",
      "show later that the probability of a slower attacker catching up\n",
      "diminishes exponentially as subsequent blocks are added. to compensate\n",
      "for increasing hardware speed and varying interest in running nodes\n",
      "over time, the proof - of - work difficulty is determined by a moving\n",
      "average targeting an average number of\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"In summary, how proof of work is doing?\"\n",
    "\n",
    "results = chroma_collection.query(query_texts=[query], n_results=5)\n",
    "retrieved_documents = results['documents'][0] # for the zero's query\n",
    "\n",
    "for document in retrieved_documents:\n",
    "    print(word_wrap(document))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv('config.env') # read local .env file for the secret key of our api. (see more security suggestion on the openAI site)\n",
    "\n",
    "openai_client = OpenAI( api_key=os.environ.get(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rag(query, retrieved_documents, model=\"gpt-4\"):\n",
    "    information = \"\\n\\n\".join(retrieved_documents)\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful expert financial research assistant. Your users are asking questions about information contained in an annual report.\"\n",
    "            \"You will be shown the user's question, and the relevant information from the annual report. Answer the user's question using only this information.\"\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": f\"Question: {query}. \\n Information: {information}\"}\n",
    "    ]\n",
    "    \n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "    )\n",
    "    content = response.choices[0].message.content\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The annual report discusses proof-of-work as a system used to implement\n",
      "a distributed timestamp server on a peer-to-peer basis. This method\n",
      "involves scanning for a specific value that, when hashed, starts with a\n",
      "number of zero bits. This system is utilized in a timestamp network and\n",
      "proof of this work can be verified by executing a single hash. The\n",
      "average work required to achieve this is exponential in the number of\n",
      "zero bits and once the effort is expended, the block can't be changed\n",
      "without redoing the work. As the system operates, it also resolves\n",
      "issues relating to representation in majority decision-making. The\n",
      "report mentions that proof-of-work makes it computationally impractical\n",
      "for an attacker to change the public history of transactions if honest\n",
      "nodes control a majority of CPU power. The system also has built-in\n",
      "checks and balances, with nodes expressing their acceptance of valid\n",
      "blocks by working to extend them and rejecting invalid blocks by\n",
      "refusing to work on them. However, the system can be vulnerable if the\n",
      "network is overpowered by an attacker.\n"
     ]
    }
   ],
   "source": [
    "llm_model = \"gpt-4\"\n",
    "output = rag(query=query, retrieved_documents=retrieved_documents, model=llm_model)\n",
    "\n",
    "print(word_wrap(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tavily LLM search engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hossein rahmani\\PycharmProjects\\CHATGPT_Prompting\\.venv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n",
      "c:\\Users\\hossein rahmani\\PycharmProjects\\CHATGPT_Prompting\\.venv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `initialize_agent` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use Use new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc. instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.utilities.tavily_search import TavilySearchAPIWrapper\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "# set up API key\n",
    "os.environ[\"TAVILY_API_KEY\"] = os.environ.get(\"TAVILY_API_KEY\")\n",
    "# set up the agent\n",
    "llm = ChatOpenAI(model_name=llm_model, temperature=0)\n",
    "search = TavilySearchAPIWrapper()\n",
    "tavily_tool = TavilySearchResults(api_wrapper=search)\n",
    "\n",
    "# initialize the agent\n",
    "agent_chain = initialize_agent(\n",
    "    [tavily_tool],\n",
    "    llm,\n",
    "    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hossein rahmani\\PycharmProjects\\CHATGPT_Prompting\\.venv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: The Proof of Work (PoW) concept in Bitcoin is a fundamental concept that ensures the security and integrity of the blockchain network. It involves miners solving complex mathematical problems, and the first one to solve it gets to add a new block to the blockchain. This process requires significant computational power and energy, making it difficult for any single entity to take control of the network. However, I can provide a more detailed explanation.\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"Proof of Work (PoW) is a consensus algorithm used in Bitcoin and other cryptocurrencies to validate transactions and add new blocks to the blockchain. In PoW, miners compete to solve complex mathematical problems using their computational resources. The first miner to solve the problem gets the right to add a new block to the blockchain and is rewarded with some amount of cryptocurrency. This process ensures the security and integrity of the blockchain, as altering any information would require redoing all the work of subsequent blocks, which is computationally impractical. However, PoW is often criticized for its high energy consumption.\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Proof of Work (PoW) is a consensus algorithm used in Bitcoin and other cryptocurrencies to validate transactions and add new blocks to the blockchain. In PoW, miners compete to solve complex mathematical problems using their computational resources. The first miner to solve the problem gets the right to add a new block to the blockchain and is rewarded with some amount of cryptocurrency. This process ensures the security and integrity of the blockchain, as altering any information would require redoing all the work of subsequent blocks, which is computationally impractical. However, PoW is often criticized for its high energy consumption.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_chain.run(\"give me basic description on proof of work concept in bitcoin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
